[2023-10-30 20:52:43,630] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-30 20:52:45,839] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-30 20:52:45,839] [INFO] [runner.py:570:main] cmd = /data/home/wangyongxin/.conda/envs/dj/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=50000 --enable_each_rank_log=None train.py --model_name_or_path /data/home/wangyongxin/examples/embedding/competition_kit/data/models/falcon-rw-1b --tokenizer /data/home/wangyongxin/examples/embedding/competition_kit/data/models/falcon-rw-1b --data_path /data/home/wangyongxin/examples/embedding/competition_kit/data/juice_data/test_1b_en.jsonl --output_dir /data/home/wangyongxin/examples/embedding/competition_kit/res_1b --per_device_train_batch_size 16 --gradient_accumulation_steps 4 --lang en --bf16 True --gradient_checkpointing_enable True --num_train_epochs 3 --model_max_length 1024 --learning_rate 1e-5 --weight_decay 0 --warmup_ratio 0.03 --evaluation_strategy no --save_strategy no --save_steps -1 --save_total_limit 999 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --deepspeed /data/home/wangyongxin/examples/embedding/competition_kit/lm-training/train_scripts/deepspeed_configs/ds_config_stage3.json
[2023-10-30 20:52:47,706] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-30 20:52:49,330] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2023-10-30 20:52:49,330] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2023-10-30 20:52:49,330] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2023-10-30 20:52:49,330] [INFO] [launch.py:163:main] dist_world_size=8
[2023-10-30 20:52:49,330] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2023-10-30 20:52:54,815] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-30 20:52:55,071] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-30 20:52:55,135] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-30 20:52:55,330] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-30 20:52:55,384] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-30 20:52:55,384] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-30 20:52:55,390] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-30 20:52:55,392] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-30 20:52:55,402] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-30 20:52:55,402] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-30 20:52:55,641] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-30 20:52:55,651] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-30 20:52:55,658] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-30 20:52:55,682] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-30 20:52:55,682] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-30 20:52:55,688] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-30 20:52:55,706] [INFO] [comm.py:637:init_distributed] cdb=None
Loading model from /data/home/wangyongxin/examples/embedding/competition_kit/data/models/falcon-rw-1b
[2023-10-30 20:53:04,969] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 292, num_elems = 1.41B
gradient_checkpointing_enable
Model class: <class 'transformers_modules.falcon-rw-1b.modeling_falcon.FalconForCausalLM'>
Tokenizer class: <class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>
+---------+---------+------------+-------------+------------+-------------+------------+-------------+------------+----------------+
|         |pad_token|pad_token_id|  bos_token  |bos_token_id|  eos_token  |eos_token_id|  unk_token  |unk_token_id|model_max_length|
+---------+---------+------------+-------------+------------+-------------+------------+-------------+------------+----------------+
|tokenizer|  [PAD]  |   50257    |<|endoftext|>|   50256    |<|endoftext|>|   50256    |<|endoftext|>|   50256    |      1024      |
+---------+---------+------------+-------------+------------+-------------+------------+-------------+------------+----------------+
|  model  |    -    |   50257    |      -      |   50256    |      -      |   50256    |      -      |   50256    |       -        |
+---------+---------+------------+-------------+------------+-------------+------------+-------------+------------+----------------+
There are 21638 training samples in data path
Formatting inputs...
Tokenizing inputs... This may take some time...
PROMPT:
"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nyour persona: as a child, i won a national spelling bee.\nyour persona: i have been published in the new yorker magazine.\nyour persona: i'm a gourmet cook.\nyour persona: i have perfect pitch.\nyour persona: i have a severe phobia of wide open spaces.\n__SILENCE__\nhey, how are you today?\npretty good and how about you?\nnot doing good. i'm agoraphobic and am afraid of wide open spaces\nthen better close your mind up\nll!!! i used to be a chef. what do you do for a living?\ni do marketing. numbers and money\nnice! i'm not good with numbers. i'm good with words and have been published.\n we can become the perfect human together\n\n### Response:"
RESPONSE:
'i won 2nd place in a spelling bee when i was 11<|endoftext|>'
====================================================================================================
PROMPT:
'Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n<p>I have an object where I wish to store separate data in</p>\n\n<pre><code>users: [\n    {\n        \'names\': [],\n        \'addresses\': [],\n    }\n],\n</code></pre>\n\n<p>I have checkboxes like so:</p>\n\n<pre><code>&lt;input type="checkbox" id="john" value="john" v-model="users.names"&gt;\n</code></pre>\n\n<p>I want to bind the checkbox with the names array.</p>\n\n<p>But it doesn\'t appear to work.</p>\n\n### Response:'
RESPONSE:
'<p>The issue is that you are trying to bind the checkbox value directly to the "names" array, which is not possible with the syntax you are using. Instead, you need to bind the checkbox value to a placeholder variable and then push that variable to the "names" array when the checkbox is selected.</p>\n\n<p>Here\'s an example of how you can achieve this:</p>\n\n<pre><code>&lt;template&gt;\n  &lt;div v-for="(user, index) in users" :key="index"&gt;\n    &lt;div&gt;User {{ index + 1 }}&lt;/div&gt;\n    &lt;div v-for="name in user.names" :key="name"&gt;{{ name }}&lt;/div&gt;\n    &lt;div&gt;\n      &lt;input type="checkbox" :id="`user-${index+1}-name`" :value="`User ${index+1}`" v-model="selectedNames"&gt;\n      &lt;label :for="`user-${index+1}-name`"&gt;Select name&lt;/label&gt;\n      &lt;button @click="addName(index)"&gt;Add name&lt;/button&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/template&gt;\n\n&lt;script&gt;\nexport default {\n  data() {\n    return {\n      selectedNames: [],\n      users: [\n        {\n          names: [],\n          addresses: []\n        },\n        {\n          names: [],\n          addresses: []\n        }\n      ]\n    }\n  },\n  methods: {\n    addName(index) {\n      this.users[index].names.push(this.selectedNames.pop());\n    }\n  }\n}\n&lt;/script&gt;\n</code></pre>\n\n<p>In this example, we are binding the checkbox value to a "selectedNames" array using v-model. When the "Add name" button is clicked, we push the selected name to the "names" array for the corresponding user. This allows us to keep the selected names separate from the "names" array until they are explicitly added by the user.</p><|endoftext|>'
====================================================================================================
PROMPT:
'Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nDo you know any other Armstrong numbers?\n\n### Response:'
RESPONSE:
'Sure. Other Armstrong numbers include `371`, `407`, `9474` and `54748`.<|endoftext|>'
====================================================================================================
PROMPT:
'Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWhat label best describes this news article?\nPhils dismiss Bowa PHILADELPHIA - Larry Bowa #39;s fiery personality was a perfect fit as a player on Philadelphia #39;s 1980 championship team. It was too much for the Phillies he managed, though, and it was one reason he was fired  \n\n### Response:'
RESPONSE:
'Sports news.<|endoftext|>'
====================================================================================================
PROMPT:
'Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nCould you explain the code?\n\n### Response:'
RESPONSE:
'Sure. The code starts with an array of integers, {1,2,3,4,5}, which is the input for the program. The program then calculates the size of the array, which is 5 in this case, and calls the reverse() function from the algorithm library. The reverse() function reverses the order of elements in the array, so the output of the program is {5,4,3,2,1}. Finally, the program prints out the reversed array.<|endoftext|>'
====================================================================================================
ORI NUMBER: 21638, AFTER FILETER: 21625, DROP NUMBER: 13
Total 21625 samples [ 5.55M tokens] in training!
Parameter Offload: Total persistent parameters: 643072 in 194 params
{'loss': 1.6689, 'learning_rate': 2.5e-06, 'epoch': 0.02}
{'loss': 1.5767, 'learning_rate': 5e-06, 'epoch': 0.05}
{'loss': 1.6343, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.07}
{'loss': 1.5608, 'learning_rate': 1e-05, 'epoch': 0.09}
{'loss': 1.5488, 'learning_rate': 9.998342337571566e-06, 'epoch': 0.12}
{'loss': 1.6462, 'learning_rate': 9.993370449424153e-06, 'epoch': 0.14}
{'loss': 1.47, 'learning_rate': 9.985087632242634e-06, 'epoch': 0.17}
{'loss': 1.4497, 'learning_rate': 9.973499378072947e-06, 'epoch': 0.19}
{'loss': 1.335, 'learning_rate': 9.958613370680507e-06, 'epoch': 0.21}
{'loss': 1.3889, 'learning_rate': 9.940439480455386e-06, 'epoch': 0.24}
{'loss': 1.4026, 'learning_rate': 9.918989757867584e-06, 'epoch': 0.26}
{'loss': 1.3113, 'learning_rate': 9.89427842547679e-06, 'epoch': 0.28}
{'loss': 1.3408, 'learning_rate': 9.866321868501914e-06, 'epoch': 0.31}
{'loss': 1.2725, 'learning_rate': 9.835138623956603e-06, 'epoch': 0.33}
{'loss': 1.2786, 'learning_rate': 9.80074936835801e-06, 'epoch': 0.36}
{'loss': 1.2728, 'learning_rate': 9.763176904016914e-06, 'epoch': 0.38}
{'loss': 1.2798, 'learning_rate': 9.722446143918307e-06, 'epoch': 0.4}
{'loss': 1.2527, 'learning_rate': 9.678584095202468e-06, 'epoch': 0.43}
{'loss': 1.2927, 'learning_rate': 9.631619841257477e-06, 'epoch': 0.45}
{'loss': 1.2551, 'learning_rate': 9.581584522435025e-06, 'epoch': 0.47}
{'loss': 1.2004, 'learning_rate': 9.528511315402358e-06, 'epoch': 0.5}
{'loss': 1.1877, 'learning_rate': 9.472435411143979e-06, 'epoch': 0.52}
{'loss': 1.2316, 'learning_rate': 9.413393991627737e-06, 'epoch': 0.54}
{'loss': 1.2203, 'learning_rate': 9.351426205150778e-06, 'epoch': 0.57}
{'loss': 1.2195, 'learning_rate': 9.286573140381663e-06, 'epoch': 0.59}
{'loss': 1.1869, 'learning_rate': 9.218877799115929e-06, 'epoch': 0.62}
[2023-10-30 20:56:45,382] [WARNING] [stage3.py:1947:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 1.151, 'learning_rate': 9.148385067763094e-06, 'epoch': 0.64}
{'loss': 1.2461, 'learning_rate': 9.075141687584056e-06, 'epoch': 0.66}
{'loss': 1.1891, 'learning_rate': 8.999196223698599e-06, 'epoch': 0.69}
{'loss': 1.1879, 'learning_rate': 8.920599032883553e-06, 'epoch': 0.71}
{'loss': 1.1779, 'learning_rate': 8.839402230183e-06, 'epoch': 0.73}
{'loss': 1.199, 'learning_rate': 8.755659654352599e-06, 'epoch': 0.76}
{'loss': 1.2126, 'learning_rate': 8.669426832160997e-06, 'epoch': 0.78}
{'loss': 1.1742, 'learning_rate': 8.580760941571968e-06, 'epoch': 0.8}
{'loss': 1.2126, 'learning_rate': 8.489720773831717e-06, 'epoch': 0.83}
{'loss': 1.1519, 'learning_rate': 8.396366694486466e-06, 'epoch': 0.85}
{'loss': 1.1589, 'learning_rate': 8.30076060335616e-06, 'epoch': 0.88}
{'loss': 1.121, 'learning_rate': 8.202965893490877e-06, 'epoch': 0.9}
{'loss': 1.2076, 'learning_rate': 8.103047409137114e-06, 'epoch': 0.92}
{'loss': 1.1544, 'learning_rate': 8.001071402741843e-06, 'epoch': 0.95}
{'loss': 1.1416, 'learning_rate': 7.897105491022819e-06, 'epoch': 0.97}
{'loss': 1.0968, 'learning_rate': 7.791218610134324e-06, 'epoch': 0.99}
{'loss': 1.1838, 'learning_rate': 7.683480969958005e-06, 'epoch': 1.02}
{'loss': 1.0385, 'learning_rate': 7.5739640075491546e-06, 'epoch': 1.04}
{'loss': 1.1665, 'learning_rate': 7.462740339769323e-06, 'epoch': 1.07}
{'loss': 1.0573, 'learning_rate': 7.349883715136601e-06, 'epoch': 1.09}
{'loss': 1.1241, 'learning_rate': 7.235468964925571e-06, 'epoch': 1.11}
{'loss': 1.1508, 'learning_rate': 7.119571953549305e-06, 'epoch': 1.14}
{'loss': 1.1125, 'learning_rate': 7.002269528256334e-06, 'epoch': 1.16}
{'loss': 1.0997, 'learning_rate': 6.883639468175926e-06, 'epoch': 1.18}
{'loss': 1.0509, 'learning_rate': 6.763760432745475e-06, 'epoch': 1.21}
{'loss': 1.0624, 'learning_rate': 6.6427119095541745e-06, 'epoch': 1.23}
{'loss': 1.1166, 'learning_rate': 6.520574161637591e-06, 'epoch': 1.25}
{'loss': 1.0287, 'learning_rate': 6.397428174258048e-06, 'epoch': 1.28}
{'loss': 1.0768, 'learning_rate': 6.273355601206143e-06, 'epoch': 1.3}
{'loss': 1.0256, 'learning_rate': 6.148438710658979e-06, 'epoch': 1.33}
{'loss': 1.0342, 'learning_rate': 6.022760330631006e-06, 'epoch': 1.35}
{'loss': 1.0115, 'learning_rate': 5.896403794053679e-06, 'epoch': 1.37}
{'loss': 1.0664, 'learning_rate': 5.76945288352031e-06, 'epoch': 1.4}
{'loss': 1.0613, 'learning_rate': 5.641991775732756e-06, 'epoch': 1.42}
{'loss': 1.0615, 'learning_rate': 5.514104985686802e-06, 'epoch': 1.44}
{'loss': 1.0359, 'learning_rate': 5.385877310633233e-06, 'epoch': 1.47}
{'loss': 1.0092, 'learning_rate': 5.257393773851733e-06, 'epoch': 1.49}
{'loss': 1.0039, 'learning_rate': 5.1287395682749444e-06, 'epoch': 1.51}
{'loss': 1.0482, 'learning_rate': 5e-06, 'epoch': 1.54}
{'loss': 1.0105, 'learning_rate': 4.871260431725058e-06, 'epoch': 1.56}
{'loss': 1.0223, 'learning_rate': 4.742606226148268e-06, 'epoch': 1.59}
{'loss': 1.0221, 'learning_rate': 4.614122689366769e-06, 'epoch': 1.61}
{'loss': 1.0024, 'learning_rate': 4.485895014313198e-06, 'epoch': 1.63}
{'loss': 1.0659, 'learning_rate': 4.358008224267245e-06, 'epoch': 1.66}
{'loss': 1.0134, 'learning_rate': 4.230547116479691e-06, 'epoch': 1.68}
{'loss': 1.0146, 'learning_rate': 4.103596205946323e-06, 'epoch': 1.7}
{'loss': 1.0367, 'learning_rate': 3.977239669368998e-06, 'epoch': 1.73}
{'loss': 1.0221, 'learning_rate': 3.851561289341023e-06, 'epoch': 1.75}
{'loss': 1.0355, 'learning_rate': 3.726644398793857e-06, 'epoch': 1.78}
{'loss': 1.0376, 'learning_rate': 3.6025718257419532e-06, 'epoch': 1.8}
{'loss': 1.0645, 'learning_rate': 3.4794258383624115e-06, 'epoch': 1.82}
{'loss': 1.0258, 'learning_rate': 3.3572880904458267e-06, 'epoch': 1.85}
{'loss': 1.0371, 'learning_rate': 3.236239567254526e-06, 'epoch': 1.87}
{'loss': 0.9723, 'learning_rate': 3.116360531824074e-06, 'epoch': 1.89}
{'loss': 1.0538, 'learning_rate': 2.997730471743667e-06, 'epoch': 1.92}
{'loss': 1.0123, 'learning_rate': 2.880428046450697e-06, 'epoch': 1.94}
{'loss': 1.0342, 'learning_rate': 2.7645310350744296e-06, 'epoch': 1.96}
{'loss': 0.9728, 'learning_rate': 2.6501162848634023e-06, 'epoch': 1.99}
{'loss': 1.0211, 'learning_rate': 2.537259660230679e-06, 'epoch': 2.01}
{'loss': 0.9757, 'learning_rate': 2.426035992450848e-06, 'epoch': 2.04}
{'loss': 1.0148, 'learning_rate': 2.316519030041998e-06, 'epoch': 2.06}
{'loss': 0.9796, 'learning_rate': 2.2087813898656775e-06, 'epoch': 2.08}
{'loss': 1.007, 'learning_rate': 2.102894508977182e-06, 'epoch': 2.11}
{'loss': 1.0143, 'learning_rate': 1.9989285972581595e-06, 'epoch': 2.13}
{'loss': 1.0144, 'learning_rate': 1.896952590862886e-06, 'epoch': 2.15}
{'loss': 0.9753, 'learning_rate': 1.7970341065091246e-06, 'epoch': 2.18}
{'loss': 0.9927, 'learning_rate': 1.699239396643841e-06, 'epoch': 2.2}
{'loss': 0.9558, 'learning_rate': 1.6036333055135345e-06, 'epoch': 2.22}
{'loss': 1.0005, 'learning_rate': 1.5102792261682813e-06, 'epoch': 2.25}
{'loss': 0.9766, 'learning_rate': 1.4192390584280347e-06, 'epoch': 2.27}
{'loss': 0.975, 'learning_rate': 1.330573167839005e-06, 'epoch': 2.3}
{'loss': 0.9337, 'learning_rate': 1.2443403456474017e-06, 'epoch': 2.32}
{'loss': 0.9362, 'learning_rate': 1.1605977698170001e-06, 'epoch': 2.34}
{'loss': 0.9438, 'learning_rate': 1.0794009671164484e-06, 'epoch': 2.37}
{'loss': 0.9816, 'learning_rate': 1.0008037763014033e-06, 'epoch': 2.39}
{'loss': 0.969, 'learning_rate': 9.248583124159438e-07, 'epoch': 2.41}
{'loss': 0.9579, 'learning_rate': 8.516149322369055e-07, 'epoch': 2.44}
{'loss': 0.9841, 'learning_rate': 7.811222008840719e-07, 'epoch': 2.46}
{'loss': 0.9626, 'learning_rate': 7.13426859618338e-07, 'epoch': 2.49}
{'loss': 0.9031, 'learning_rate': 6.485737948492237e-07, 'epoch': 2.51}
{'loss': 0.9612, 'learning_rate': 5.866060083722624e-07, 'epoch': 2.53}
{'loss': 0.9579, 'learning_rate': 5.275645888560233e-07, 'epoch': 2.56}
{'loss': 0.958, 'learning_rate': 4.71488684597643e-07, 'epoch': 2.58}
{'loss': 0.9679, 'learning_rate': 4.184154775649768e-07, 'epoch': 2.6}
{'loss': 0.9301, 'learning_rate': 3.683801587425251e-07, 'epoch': 2.63}
{'loss': 0.9745, 'learning_rate': 3.214159047975324e-07, 'epoch': 2.65}
{'loss': 0.9568, 'learning_rate': 2.7755385608169374e-07, 'epoch': 2.67}
{'loss': 0.9778, 'learning_rate': 2.368230959830875e-07, 'epoch': 2.7}
{'loss': 0.9855, 'learning_rate': 1.992506316419912e-07, 'epoch': 2.72}
{'loss': 0.9712, 'learning_rate': 1.6486137604339813e-07, 'epoch': 2.75}
{'loss': 0.963, 'learning_rate': 1.3367813149808728e-07, 'epoch': 2.77}
{'loss': 1.0055, 'learning_rate': 1.0572157452321097e-07, 'epoch': 2.79}
{'loss': 0.99, 'learning_rate': 8.101024213241826e-08, 'epoch': 2.82}
{'loss': 1.009, 'learning_rate': 5.9560519544614725e-08, 'epoch': 2.84}
{'loss': 0.9879, 'learning_rate': 4.138662931949255e-08, 'epoch': 2.86}
{'loss': 0.9207, 'learning_rate': 2.6500621927054716e-08, 'epoch': 2.89}
{'loss': 0.9764, 'learning_rate': 1.4912367757366485e-08, 'epoch': 2.91}
{'loss': 0.9866, 'learning_rate': 6.629550575847355e-09, 'epoch': 2.93}
{'loss': 0.9968, 'learning_rate': 1.657662428434792e-09, 'epoch': 2.96}
{'loss': 0.9553, 'learning_rate': 0.0, 'epoch': 2.98}
{'train_runtime': 957.0032, 'train_samples_per_second': 67.79, 'train_steps_per_second': 0.132, 'train_loss': 1.1052081395709326, 'epoch': 2.98}
[2023-10-30 21:09:15,471] [INFO] [launch.py:347:main] Process 1975640 exits successfully.
[2023-10-30 21:09:15,471] [INFO] [launch.py:347:main] Process 1975641 exits successfully.
[2023-10-30 21:09:15,471] [INFO] [launch.py:347:main] Process 1975635 exits successfully.
[2023-10-30 21:09:15,471] [INFO] [launch.py:347:main] Process 1975636 exits successfully.
Finish training...
[2023-10-30 21:09:16,473] [INFO] [launch.py:347:main] Process 1975638 exits successfully.
[2023-10-30 21:09:16,473] [INFO] [launch.py:347:main] Process 1975639 exits successfully.
[2023-10-30 21:09:16,473] [INFO] [launch.py:347:main] Process 1975637 exits successfully.
[2023-10-30 21:09:17,474] [INFO] [launch.py:347:main] Process 1975634 exits successfully.
